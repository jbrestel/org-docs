#+STARTUP: indent

* Redmine PROJECT
:PROPERTIES:
:query_id: 298
:query_type: PROJECT
:END:
** UniDB
:PROPERTIES:
:project_identifier: jbdqh
:project_name: UniDB
:project_id: 2011
:END:
[[https://redmine.apidb.org/projects/jbdqh][Project]]
[[https://redmine.apidb.org/projects/jbdqh/issues/new][New Issue]]
*** Description
#+BEGIN_DESCRIPTION

#+END_DESCRIPTION


* B53 :JohnB:
** Workflow Changes
- [X] Remove step copy to webservices and download files
** Schema Changes
- [ ] study.study study_id and investigation_id need to be number(8)
- [X] Add "database_orig" and "primary_key_orig" fields to every table in "Core.TableInfo"
- [X] Add Index on each table for 3 fields (pk, pk_orig, db_orig)
- [X] new script to add fields after apidbschema has been made.  last script when creating apidb schema
  #+begin_src sql
   BEGIN
   FOR item in (select di.name || '.' || di.name as table_name
                     , ti.primary_key_column
                     , di.name || '_' || di.name || '_map' as index_name
                     from (-- everything but version and userdataset schemas
                     select  t.*
                     from core.tableinfo t, core.databaseinfo d
                     where lower(t.table_type) != 'version'
                     and t.DATABASE_ID = d.DATABASE_ID
                     and d.name not in ('UserDatasets', 'ApidbUserDatasets', 'chEBI', 'hmdb')
                     and t.name not in ('AlgorithmParam','GlobalNaturalKey','DatabaseTableMapping','SnpLinkage', 'CompoundPeaksChebi')
                     minus
                     -- minus Views on tables
                     select * from core.tableinfo where view_on_table_id is not null
                     ) ti, core.databaseinfo di
                     where ti.database_id = di.database_id   )
   LOOP
    ALTER table item.table_name add database_orig varchar2(30), primary_key_orig number(20);
    CREATE INDEX index_name ON item.table_name (database_orig, primary_key_orig, item.primary_key_column) tablespace indx; 
   END LOOP;
   END;
  #+end_src
** ApiCommonDatasts
- [X] configure only dbs which have been exported for b53
- [X] bld ApiCommonDatasts and generateFromDatasets
** Plugin Changes
- [X] Skip ApiDB.SequenceVariation
  - ~ 58 hours to load 633 mil rows from Plasmo (fungi has > 1.3 billion rows)
    #+BEGIN_EXAMPLE
    Sun Apr  4 18:22:17 2021	Begin ID Lookup for ApiDB::SequenceVariation from database plas051n
    Sun Apr  4 18:24:35 2021	Finished ID Lookup for ApiDB::SequenceVariation from database plas051n
    Sun Apr  4 18:29:19 2021	Processed 100000 from ApiDB::SequenceVariation
    Sun Apr  4 18:29:55 2021	Processed 200000 from ApiDB::SequenceVariation
    ....
    Wed Apr  7 05:30:14 2021	Processed 633000000 from ApiDB::SequenceVariation
    Wed Apr  7 05:30:44 2021	Processed 633100000 from ApiDB::SequenceVariation
    Wed Apr  7 05:30:48 2021	Finished Reading data from ApiDB::SequenceVariation
    Wed Apr  7 05:30:48 2021	Finished Loading 633110728 Rows into table ApiDB::SequenceVariation from database plas051n
    #+END_EXAMPLE
** Tuning Manager :JohnI:
- [ ] SNPAttributes (ok for b53 but will be missing some fields)
- [ ] SNPStrains (update sql so it doesn't use SequenceVariation)
  #+begin_src sql
	select * from study.protocolappnode where name like '% (Sequence Variation)'
  #+end_src
** Model
- [ ] Using include/exclude projects make the snpattributes which are missing internal for b53

* B54 :JohnB:Wei:
** Genomics Workflow Changes
- [ ] Do not load ApiDB.SequenceVariation.  Instead add JSON clob to ApiDB.SNP for the variations which can be used to make the snp record page table
- [ ] Add other 2 SNP Attributes which are being added by tuning manager
- [ ] Required rebuild for all snp datasets 
** Schema Changes
- [ ] use perl script to automatically add "database_orig" and "primary_key_orig" in installer
- [ ] add 3 new fields to SRes.OntologySynonym
** Plugin Changes
- [ ] Database Mapping Table
  - [ ] Global Rows still need to populate existing Database Mapping Table
  - [ ] Component Specific Rows should populate the 2 new fields in the primary table
  - [ ] Undo needs to work BOTH ways because of legacy data in genomics sites
  - [ ] Existing queries using databasemapping table need to be union of both ways because of legacy data in genomics sites
- [ ] Address cleanup steps for failures
  - [ ] Database Specific / Non Global we get for free as there is no clean up needed after above change
  - [ ] For Global Rows, we must delete the apidb.databasetablemapping rows for this table + alg_invocation
    #+begin_src sql
	  delete apidb.databasetablemapping where database_orig = ? and table_name = ? and row_alg_invocation_id = ?
    #+end_src
  - [ ] update how we find unidb aware tables
    #+begin_src sql
      select ti.name as table_name
           , di.name as database_name
           , ti.primary_key_column
      from core.tableinfo ti, core.databaseinfo di,
           (select owner, table_name
           from all_tab_columns 
           where column_name in ('DATABASE_ORIG', 'PRIMARY_KEY_ORIG')
           and owner != 'EDA' -- TEMPORARY
           group by owner, table_name
           having count(*) = 2) wit
      where ti.database_id = di.database_id
      and upper(wit.owner) = upper(di.name)
      and upper(wit.table_name) = upper(ti.name)
      and ti.VIEW_ON_TABLE_ID is null
    #+end_src

* Future Releases :JohnI:Lin:
- How to update Taxonomy?
  - should unidb workflow load one version of taxonomy instead of pulling from other dbs?
- wgcna (host /pathogen)

* QA for first release
** Path to fix database for missing data :JohnB:
- [X] Confirm problem due to missed undo of fung-inc an plas-inc
- [X] do inc addition for plas and fung
- [X] (may take up to a week?) Rerun tuning manager
- [X] webservices?? probably not an issue
- rerun tuning manager for intronjunctions
** Manual QA :Bindu:Cristina:JohnI:JohnB:Mark:
- [X] organisms and datasets missing (~500 / 516?)
  - theory is missed fung and plas inc addition for b52
- [ ] (Cristina) [Reference] display in org param (client side)
- [ ] replace use of @PROJECT_ID@ macro with display name for the project
- [ ] initial qa resulted in some redmines [[https://redmine.apidb.org/issues/12567][Redmines]]
- [ ] includes/excludes  for project specific searches and tables.  this should be handled by automated tests
  - [ ] Check SignalP, Interpro, ...
  - [ ] Look for the same sqlQuery but with different sql (include/exclude)
- [ ] Any searches/record tables which are site specific need to be
- [ ] Focus on gene page
  - table content (compare component to uni)
- [ ] gene model characteristics / filter param not returning
** Automated QA Process :Mark:JohnI:JohnB:Bindu:Lin:

- [ ] compare list of wdk Searches, Attributes, and Tables in UniDB to union of all component projects
  - [ ] Record expected and Actual Results
- [ ] Selenium Tests for Search Pages
- [-] use the webserive queries for Performance Tests
  - [X] compare plasmodb.org (b52) to feature.veupathdb.org
  - [ ] address faiures
  - [ ] Record expected and Actual Results
- [-] attribute tables
  - [X] using sql minus operation, compare 052 attribute tables in eupa052n to uni-inc
  - [ ] address faiures
  - [ ] Record expected and Actual Results
- [-] Record page tables;  can test against individual components
  - [X] foreach record table, run the sql and compare counts (union for all components)
  - [ ] address faiures
  - [ ] Record expected and Actual Results
- [ ] Run all Param queries on uni052n vs union of components
  - [ ] Deal with enum params
  - [ ] is there a webservice way to get the values for all params
** Classes of Problems
- [ ] :Bindu: Webservice Issues (hsss, motif, profile similarity)
- [ ] includes/excludes

** Performance
- punt
** Site Search
- indexes build from b52 instances
- (Cristina) change feature site to use prod solr (same for blast and buildNumber in the model should be 52)
** JBrowse
    + intronjunction tuning table 

* Use Cases For Unified Database
- fully functional portal (veupathdb.org)
- host / pathogen / vector
- user defined organism preferences
- Fewer prod db instances
- ebi2gus

* Basics
- [[https://docs.google.com/document/d/1K3ckE6hwN9r-Dp1Av_zDH5Jcr7ApbBjQ-7yJM1zk0bQ/edit][UniDB Design Document December 2019]]
- [[https://wiki.apidb.org/index.php/UniDB%20Workflow][Wiki / Technical info about running workflow]]

* Table Reader(s)
- [[~/project_home/ApiCommonData/Load/lib/perl/UniDBTableReader.pm][UniDB Table Reader]]
  - initial thinking was that sql queries would map input->output
- [[~/project_home/ApiCommonData/Load/lib/perl/GUSTableReader.pm][GUSTableReader]]
  - inpput is a GUS oracle instance
  - queries here are like "select * from $table"
- [[~/project_home/ApiCommonData/Load/lib/perl/EBITableReader.pm][EBI Table Reader]]
  - input is flat files
  - no queries.  the translation from chado/mysql->GUS is done via [[https://github.com/VEuPathDB/ebi2gus/][ebi2gus]] and uses the E! perl API
    - [[~/project_home/ebi2gus/Dockerfile]]
    - [[~/project_home/ebi2gus/lib/perl/EBIParser.pm]]
    - [[~/project_home/ebi2gus/lib/perl/GUS/DoTS/GeneFeature.pm]]

* ebi2GUS
- given mysql dump (init.sql), the container will fire up mariadb server, run the init.sql, will run dumpGUS.pl which creates a set of files (one file per GUS Table)
- new branch needed
  - change to GUS schema ( ~ 40 tables for "core genome")
  - changes to input schema
  - changes to the perl api

